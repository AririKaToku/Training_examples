{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import mean_squared_error\n",
    "import keras.backend as K\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressViewer():\n",
    "    def __init__(self, create_new_directory = True):\n",
    "        self.losses = []\n",
    "        self.rewards = []\n",
    "        self.epsilons = []     \n",
    "        \n",
    "        if create_new_directory:\n",
    "            self.__path = self.__make_dir()\n",
    "    \n",
    "    def add(self, reward, epsilon, loss):\n",
    "        \n",
    "        self.add_reward(reward)\n",
    "        self.add_epsilon(epsilon)\n",
    "        self.add_loss(loss)\n",
    "        \n",
    "        if len(self.rewards) > 99:\n",
    "            self.__save_array()    \n",
    "            self.rewards = []\n",
    "            self.epsilons = []\n",
    "            self.losses = []\n",
    "    \n",
    "    def add_reward(self, value):\n",
    "        \n",
    "        self.rewards.append(value)\n",
    "        \n",
    "    def add_loss(self, value):\n",
    "        self.losses.append(value)\n",
    "    \n",
    "    def add_epsilon(self, value):\n",
    "        self.epsilons.append(value)\n",
    "    \n",
    "    def get_rewards(self):\n",
    "        return self.rewards\n",
    "    \n",
    "    def get_epsilons(self):\n",
    "        return self.epsilons\n",
    "    \n",
    "    def plot_rewards(self, begin = 0, end = -1):\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.plot(self.rewards)\n",
    "    \n",
    "    def plot_epsilons(self, begin = 0, end = -1):\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.plot(self.epsilons)\n",
    "        \n",
    "    def plot_lossess(self, begin = 0, end = -1):\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.plot(self.losses[begin:end])\n",
    "\n",
    "    def plot_mean_rewards(self, mean = 10):\n",
    "        plt.figure(figsize=(20,10))\n",
    "        mean_rewards = []\n",
    "        for i in range(0,len(self.rewards)-mean,mean):\n",
    "            mean_rewards.append(np.mean(self.rewards[i * mean : (i+1) * mean]))\n",
    "        plt.plot(mean_rewards)\n",
    "    \n",
    "    def plot(self, path = None):\n",
    "        \"\"\"\n",
    "        отрисовывает график награды, изменения функции потерь и epsilon,\n",
    "        а также выводит все графики на одном\n",
    "        \"\"\"\n",
    "        x,y,z = self.__load_array(path)\n",
    "        \n",
    "        x1 = np.array(x) / np.max(x)\n",
    "        y1 = np.array(y) / np.max(y)\n",
    "        z1 = np.array(z) / np.max(z)\n",
    "\n",
    "        grid = plt.GridSpec(2, 3)\n",
    "\n",
    "        plt.figure(figsize=(20,10))\n",
    "\n",
    "        q1 = plt.subplot(grid[0,0])\n",
    "        q2 = plt.subplot(grid[0,1])\n",
    "        q3 = plt.subplot(grid[0,2])\n",
    "        q4 = plt.subplot(grid[1,:3])\n",
    "\n",
    "        q1.plot(x, color = 'coral', label = 'reward')\n",
    "        q1.legend()\n",
    "\n",
    "        q2.plot(y, color = 'coral', label = 'loss')\n",
    "        q2.legend()\n",
    "\n",
    "        q3.plot(z, color = 'coral', label = 'epsilon')\n",
    "        q3.legend()\n",
    "\n",
    "        q4.plot(x1, color = 'coral', label = 'reward')\n",
    "        q4.plot(y1, color = 'skyblue', label = 'loss')\n",
    "        q4.plot(z1, color = 'grey', label = 'epsilon')\n",
    "        q4.legend()\n",
    "        \n",
    "    def __save_array(self):\n",
    "        \"\"\"\n",
    "        сохраняет все массивы в файл чтобы не забиваться память, \n",
    "        файл создается в директории, созданной методом makedir автоматически\n",
    "        \"\"\"\n",
    "        filename = str(datetime.datetime.now().time()).replace(':','-')+'.visualiser'\n",
    "        with open(self.__path+'//'+filename,'wb') as file:\n",
    "            pickle.dump([self.rewards,\n",
    "                         self.epsilons,\n",
    "                         self.losses \n",
    "                         ],\n",
    "                        file)\n",
    "        \n",
    "    def __make_dir(self):\n",
    "        \"\"\"\n",
    "        создает автоматически дмректорию, куда будут сохраняться все файлы лога\n",
    "        \"\"\"\n",
    "        path = 'viewer_log_'+str(datetime.datetime.now())[:16].replace(':','-')\n",
    "        os.mkdir(path)\n",
    "        return path\n",
    "    \n",
    "    def __load_array(self, path = None):\n",
    "        \"\"\"\n",
    "        из директории, созданной makedir, собирает по всем файлам в директории \n",
    "        единые списки наград, ошибок и эпсилона (для отрисовки)\n",
    "        \"\"\"\n",
    "        if path == None:\n",
    "            path_to_log = self.__path\n",
    "        else:\n",
    "            path_to_log = path\n",
    "            \n",
    "        files = os.listdir(path_to_log)\n",
    "        res_list = []\n",
    "        \n",
    "        for file in files:\n",
    "            with open(path_to_log+'//'+file, 'rb') as f:\n",
    "                var_list = pickle.load(f)    \n",
    "                if len(res_list) == 0:\n",
    "                    res_list = var_list.copy()\n",
    "                else:\n",
    "                    res_list[0] += var_list[0]\n",
    "                    res_list[1] += var_list[1]\n",
    "                    res_list[2] += var_list[2]\n",
    "\n",
    "        return res_list[0],res_list[1],res_list[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, env):\n",
    "        self.env     = env\n",
    "        self.memory  = deque(maxlen=2000)\n",
    "        \n",
    "        self.gamma = 0.85\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.9995\n",
    "        self.learning_rate = 0.001\n",
    "        self.tau = 0.95 # если тау 1 то веса из обучаемой сети в целевую копируются полностью, если 0 то не копируются\n",
    "\n",
    "        self.model        = self.create_model() #сеть, которая учит функцию ценности на взаимодействии со средой\n",
    "        self.target_model = self.create_model() #сеть, которая предсказывает функцию ценности\n",
    "        \n",
    "        self.history = 0 #нужно для мониторинга функции потерь\n",
    "\n",
    "    def create_model(self):\n",
    "        model   = Sequential()\n",
    "        state_shape  = self.env.observation_space.shape\n",
    "        model.add(Dense(24, input_dim=state_shape[0], activation=\"relu\"))\n",
    "        model.add(Dense(48, activation=\"relu\"))\n",
    "        model.add(Dense(24, activation=\"relu\"))\n",
    "        model.add(Dense(self.env.action_space.n))\n",
    "        model.compile(loss=mean_squared_error,\n",
    "            optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.append([state, action, reward, new_state, done])\n",
    "\n",
    "    def replay(self):\n",
    "        batch_size = 32\n",
    "        if len(self.memory) < batch_size: \n",
    "            return\n",
    "\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        for sample in samples:\n",
    "            state, action, reward, new_state, done = sample\n",
    "            target = self.target_model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                Q_future = max(self.target_model.predict(new_state)[0])\n",
    "                target[0][action] = reward + Q_future * self.gamma\n",
    "            history = self.model.fit(state, target, epochs=1, verbose=0)\n",
    "            self.history = history.history['loss'][0]\n",
    "\n",
    "    def target_train(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)\n",
    "        self.target_model.set_weights(target_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env     = gym.make(\"CartPole-v1\")\n",
    "gamma   = 0.9\n",
    "epsilon = .95\n",
    "\n",
    "trials  = 1000000\n",
    "trial_len = 500\n",
    "\n",
    "updateTargetNetwork = 500\n",
    "dqn_agent = DQN(env=env)\n",
    "viewer = ProgressViewer(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial:0, mean_reward:24.0\n",
      "trial:10, mean_reward:22.3\n",
      "trial:20, mean_reward:21.8\n",
      "trial:30, mean_reward:13.5\n",
      "trial:40, mean_reward:23.0\n",
      "trial:50, mean_reward:19.7\n",
      "trial:60, mean_reward:13.1\n",
      "trial:70, mean_reward:17.9\n",
      "trial:80, mean_reward:21.4\n",
      "trial:90, mean_reward:15.9\n",
      "trial:100, mean_reward:12.0\n",
      "trial:110, mean_reward:25.9\n",
      "trial:120, mean_reward:22.6\n",
      "trial:130, mean_reward:30.9\n",
      "trial:140, mean_reward:29.3\n",
      "trial:150, mean_reward:19.4\n",
      "trial:160, mean_reward:12.6\n",
      "trial:170, mean_reward:26.0\n",
      "trial:180, mean_reward:44.6\n",
      "trial:190, mean_reward:51.2\n",
      "trial:200, mean_reward:28.0\n",
      "trial:210, mean_reward:112.3\n",
      "trial:220, mean_reward:138.2\n",
      "trial:230, mean_reward:181.1\n",
      "trial:240, mean_reward:158.3\n",
      "trial:250, mean_reward:193.3\n",
      "trial:260, mean_reward:173.2\n",
      "trial:270, mean_reward:159.8\n",
      "trial:280, mean_reward:117.8\n",
      "trial:290, mean_reward:117.1\n",
      "trial:300, mean_reward:42.0\n",
      "trial:310, mean_reward:119.4\n",
      "trial:320, mean_reward:110.3\n",
      "trial:330, mean_reward:64.9\n",
      "trial:340, mean_reward:81.1\n",
      "trial:350, mean_reward:54.4\n",
      "trial:360, mean_reward:70.1\n",
      "trial:370, mean_reward:17.1\n",
      "trial:380, mean_reward:61.4\n",
      "trial:390, mean_reward:74.9\n",
      "trial:400, mean_reward:42.0\n",
      "trial:410, mean_reward:113.5\n",
      "trial:420, mean_reward:115.5\n",
      "trial:430, mean_reward:221.2\n",
      "trial:440, mean_reward:175.4\n",
      "trial:450, mean_reward:207.3\n",
      "trial:460, mean_reward:166.8\n"
     ]
    }
   ],
   "source": [
    "update_step_counter = 0\n",
    "for trial in range(trials):\n",
    "    cur_state = env.reset().reshape(1,4)\n",
    "    total_episode_reward = 0\n",
    "    \n",
    "    for step in range(trial_len):\n",
    "        action = dqn_agent.act(cur_state)\n",
    "        \n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        total_episode_reward += reward\n",
    "        \n",
    "        reward = reward if not done else -20\n",
    "        \n",
    "        new_state = new_state.reshape(1,4)\n",
    "        dqn_agent.remember(cur_state, action, reward, new_state, done)\n",
    "\n",
    "        dqn_agent.replay()       # internally iterates default (prediction) model\n",
    "#         dqn_agent.target_train() # iterates target model\n",
    "        update_step_counter += 1\n",
    "        if update_step_counter == updateTargetNetwork:\n",
    "            dqn_agent.target_train()\n",
    "            update_step_counter = 0\n",
    "\n",
    "        cur_state = new_state\n",
    "        if done:\n",
    "            viewer.add(step, dqn_agent.epsilon, dqn_agent.history)\n",
    "            break        \n",
    "    if trial % 10 == 0:\n",
    "        print('trial:{}, mean_reward:{}'.format(trial, np.mean(viewer.rewards[-10:])))\n",
    "#         print('trial:{}, mean_reward:{}'.format(trial, total_episode_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
